{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3bfba1-57b1-4a11-91a2-c5ea70158ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow \n",
    "# if not installed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff02536-d913-44eb-816e-2a4b895f0422",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5aa815a-16f5-4a01-9bde-0897256a6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models # for layering and compiling\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adamax # opitimizers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d16de-fd26-4306-adb1-e0d56e8348f0",
   "metadata": {},
   "source": [
    "### Load the Dataset (Build in keras MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fbad7ee5-5e01-47d4-b096-1144b12f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist # Load the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7bc9d-3b3d-4a77-b831-5b7db2103961",
   "metadata": {},
   "source": [
    "### Pre-Process the dataset and SPLIT them into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5bec8a71-fbfc-4984-b1a2-ab584ab2217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b1051e8-207e-4f33-ab2d-195d278f47cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b20da80b-431c-46cf-9db7-1e9b820058dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bebdf-1297-47d4-9a86-456a51c355c9",
   "metadata": {},
   "source": [
    "# Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f911cea6-7e6c-4f23-a855-d193e16b0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pixel: 255\n",
      "Min Pixel: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Max Pixel:',train_images.max()) # Checking highest pixel values for train_images\n",
    "print('Min Pixel:',train_images.min()) # Checking min pixel values for train_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7057ab2c-978d-40fb-87aa-0e3a4dbe203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pixel: 255\n",
      "Min Pixel: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Max Pixel:',test_images.max()) # Checking highest pixel values for test_images\n",
    "print('Min Pixel:',test_images.min()) #Checking min pixel values for test_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99952808-3dec-4ded-90bb-d8680d35fe99",
   "metadata": {},
   "source": [
    "### Here we normalize the pixel values of the image by dividing train & test image sets to 255\n",
    "So that the range of pixel value stays in between 0 to 1  \n",
    "It will help model to learn faster and efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac44d581-785d-4e0f-b8f4-79ec2861de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images = train_images/255 # divided train_images with highest value (255) to normalize \n",
    "test_images = test_images/255 # divided test_images with highest value (255) to normalize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c6b15c93-0688-4f5f-97ec-63e90c0aa900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pixel: 1.0\n",
      "Min Pixel: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Again check after normalize train_images\n",
    "print('Max Pixel:',train_images.max()) \n",
    "print('Min Pixel:',train_images.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "936387fe-9887-49e5-9c4a-dce1027368e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pixel: 1.0\n",
      "Min Pixel: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Again check after normalize test_images\n",
    "print('Max Pixel:',test_images.max())\n",
    "print('Min Pixel:',test_images.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89e21b-f118-408b-95bb-30f4d6a79dba",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c1a98-9158-4fb5-b19f-9f5d0b7d7c00",
   "metadata": {},
   "source": [
    "\n",
    "model = models.Sequential([  \n",
    "    layers.Flatten(input_shape = (28,28)),  **make the 28 x 28 array into 1D array**  \n",
    "    layers.Dense(128, activation = 'relu'), **128 hidden layers commonly used we can adjust it if needed**  \n",
    "    layers.Dense(10, activation='softmax') **here 10 is output layer. thus, MNIST has 10 classes**   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b94ba-25b5-417e-9f93-b4bd2046871f",
   "metadata": {},
   "source": [
    "**Defining some OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6449c825-6623-4ee7-a546-54fd8d6a15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizers = [SGD(), Adam(), RMSprop(), Adamax()] # Storing some optimizer in a list to execute at a time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1632b-2f5a-4329-9970-687a4d9030ff",
   "metadata": {},
   "source": [
    "### Compiling the model and Store the data in List \n",
    "accuracy_list  \n",
    "loss_list  \n",
    "**loss function = 'sparse_categorical_crossentropy'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b4d60bef-2e38-42d7-97be-7b26f9466df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = [] # to store accuracy\n",
    "loss_list = [] # to store loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4ea1167a-2b5a-479e-8603-cd0a4f72441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1605 - accuracy: 0.9536\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9774\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.9788\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0870 - accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "for optimizer in optimizers:\n",
    "    # build the neural network format for especially MNIST dataset\n",
    "    model = models.Sequential([\n",
    "                layers.Flatten(input_shape = (28,28)),\n",
    "                layers.Dense(128, activation = 'relu'),\n",
    "                layers.Dense(10, activation='softmax') \n",
    "            ])\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = 'sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # fit the models\n",
    "    model.fit(train_images, train_labels, epochs=10, verbose=0)\n",
    "    \n",
    "    loss_metric, accuracy = model.evaluate(test_images,test_labels)\n",
    "\n",
    "    # storing accuracy and loss metric for evaluate later\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(loss_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803a64-22f9-475c-8694-2c1cdea111ed",
   "metadata": {},
   "source": [
    "# Evaluate the Loss Function and Accuracy on behalf of different OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "27c60ab6-3af6-4fdb-8632-946aa8feb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZER:  SGD \tLoss Metric:  0.16052822768688202 \tAccuracy:  0.9535999894142151\n",
      "OPTIMIZER:  Adam \tLoss Metric:  0.07821965217590332 \tAccuracy:  0.977400004863739\n",
      "OPTIMIZER:  RMSprop \tLoss Metric:  0.08780968934297562 \tAccuracy:  0.9787999987602234\n",
      "OPTIMIZER:  Adamax \tLoss Metric:  0.08700641244649887 \tAccuracy:  0.9750999808311462\n"
     ]
    }
   ],
   "source": [
    "opt =['SGD', 'Adam', 'RMSprop', 'Adamax']\n",
    "\n",
    "for i in range(len(opt)):\n",
    "    print('OPTIMIZER: ',opt[i], '\\tLoss Metric: ',loss_list[i], '\\tAccuracy: ', accuracy_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a93576-6e64-4145-b0cb-1a117686be41",
   "metadata": {},
   "source": [
    "### Here after evaluating we can see using KERAS simple neuron & also among the various optimizer ADAM optimizer overall performs better in terms of ACCURACY & Loss Metric. Though RMSprop optimizers ACCURACY is a bit high but ADAM loss metric is performed better here  \n",
    "# Finally we can say in this network ADAM optimizer is best for the dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1648569-4a03-4071-ba25-fe6c5dc44f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a986b5-d85a-43da-8860-831e7b0bcb66",
   "metadata": {},
   "source": [
    "# Description how I configure the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85674439-2168-4cb2-a86c-9cfe7ab6a7ca",
   "metadata": {},
   "source": [
    "As per the intruction I have to build a simple neuron network which can classify images from MNIST dataset for that -   \n",
    "*Firstly*, I imported the necessary libraries like tensorflow and keras layers, models and optimizer.    \n",
    "\n",
    "*Secondly*, I load the dataset and split it into train,test.    \n",
    "\n",
    "Then I check the shape of the dataset and find it's maximum and minimum pixel and after that, I normalized the pixels range into 0 to 1 by divide them their highest value.  \n",
    "\n",
    "After prepossessing I have build the network to train the dataset. First of all to handle dataset like MNIST generally we do not go to very deep so I do not use layers like Conv2D, Maxpool etc.    \n",
    "\n",
    "I kept the model simple.  \n",
    "I used **Flatten** to make the 28 x 28 array into 1D array.  \n",
    "\n",
    "For hidden layer dense I used 128. Here we can use other number of layers too. Used **ReLu** activation function for input layer. on the reason to use **ReLu** is It doesn't allow for the activation of all of the neurons at the same time and it keeps the input value in range 0 to 1.  \n",
    "\n",
    "For output layer I used 10 because MNIST dataset has 10 classes. Here Activation function used **softmax**. Softmax in the output layer is suitable for multi-class classification problems, providing a probability distribution over the different classes. I ignore **Sigmoid** because it generally used for binary classification problem.\n",
    "\n",
    "Before compile the model I have created a list called OPTIMIZER to store the opimizers I have imported from keras to execute them at once using loop. I used four optimizer **SGD, Adam, RMSprop, Adamax** and then I used **sparse_categorical_crossentropy** as a loss function. Here I also tried to use **MeanSquaredError** but then I got to know It's typically used for regression model so I continue with **sparse_categorical_crossentropy**.\n",
    "\n",
    "After training/ fit the model using for different optimizer I evaluate that overall **Adam** optimizer performs better   \n",
    "It's **Loss Metric = 0.07821965217590332 & Accuracy = 0.977400004863739**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858b9e3-69c3-49ec-9e60-f63aa61c8cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
